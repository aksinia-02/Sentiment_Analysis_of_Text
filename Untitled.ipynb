{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cde010a-2b60-4a1b-8e7d-e6d124548240",
   "metadata": {},
   "source": [
    "# Project 4: Sentiment Analysis of Text\n",
    "\n",
    "This project involves building and evaluating deep learning models (RNNs or Transformers) for sentiment classification of text, such as movie reviews or product feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661f799-04cc-477d-a497-4267ff78d464",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e848d9e3-fc5a-47ff-b29c-037a079cb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch Core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774877db-1d9a-4a1e-99fb-c0420aca9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79038126-4c30-4196-ba85-82ee0f67ce48",
   "metadata": {},
   "source": [
    "## Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4e2cb1-a156-43d9-9b6c-8a13e216b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e06bece-76d2-4c86-80b6-c4c61f66e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(dataset: str, path: str):\n",
    "    os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_download_files(dataset, path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abdd1ecf-8d03-4dd5-af3f-49b997eb393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_label(label, name):\n",
    "    if label == -1 or label == 'negative':\n",
    "        return 0\n",
    "    elif label == 1 or label == 'positive':\n",
    "        return 2\n",
    "    elif label == 0 and name != \"SST-2\":\n",
    "        return 1\n",
    "    # negative for SST-2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69d98439-c7ac-45fd-8fc1-b74c047042c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(path: str, label: str, text: str, delimiter= \",\", name=\"\"):\n",
    "    df = pd.read_csv(path,delimiter=delimiter)\n",
    "\n",
    "    df = df.rename(columns={text: \"text\", label: \"label\"})\n",
    "\n",
    "    df['label'] = df['label'].apply(lambda x: remap_label(x, name))\n",
    "\n",
    "    num_labels = df['label'].nunique()\n",
    "    print(f\"Number of distinct labels: {num_labels}\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd423-fee5-45d3-b413-abe4b2e47180",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35d20b5c-1361-42d1-99f1-8e2a6f01e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n"
     ]
    }
   ],
   "source": [
    "download_dataset('lakshmi25npathi/imdb-dataset-of-50k-movie-reviews', 'IMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3144023-ebb1-4fca-9693-d16ca68d2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct labels: 2\n",
      "                                                text  label\n",
      "0  One of the other reviewers has mentioned that ...      2\n",
      "1  A wonderful little production. <br /><br />The...      2\n",
      "2  I thought this was a wonderful way to spend ti...      2\n",
      "3  Basically there's a family where a little boy ...      0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...      2\n",
      "Number of rows: 50000\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\"IMDB\", \"IMDB Dataset.csv\")\n",
    "imdb = show_data(path, \"sentiment\", \"review\", name=\"IMDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9adc7-b04c-4f0e-ae45-58ea584ea0fb",
   "metadata": {},
   "source": [
    "### SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4d87da9-537a-469a-b325-ebbeb897b1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/jkhanbk1/sst2-dataset\n"
     ]
    }
   ],
   "source": [
    "download_dataset('jkhanbk1/sst2-dataset', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "349e92b9-3b1f-47d2-914d-82deb2d7678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: ['train.csv', 'val.csv', 'test.csv']\n",
      "Number of distinct labels: 2\n",
      "   label                                               text\n",
      "0      2  A stirring, funny and finally transporting re-...\n",
      "1      0  Apparently reassembled from the cutting-room f...\n",
      "2      0  They presume their audience won't sit still fo...\n",
      "3      2  This is a visually stunning rumination on love...\n",
      "4      2  Jonathan Parker's Bartleby should have been th...\n",
      "Number of rows: 6920\n",
      "split: ['train.csv', 'val.csv', 'test.csv']\n",
      "Number of distinct labels: 2\n",
      "   label                                               text\n",
      "0      0                        One long string of cliches.\n",
      "1      0  If you've ever entertained the notion of doing...\n",
      "2      0  K-19 exploits our substantial collective fear ...\n",
      "3      0  It's played in the most straight-faced fashion...\n",
      "4      2  There is a fabric of complex ideas here, and f...\n",
      "Number of rows: 872\n",
      "split: ['train.csv', 'val.csv', 'test.csv']\n",
      "Number of distinct labels: 2\n",
      "   label                                               text\n",
      "0      0        No movement, no yuks, not much of anything.\n",
      "1      0  A gob of drivel so sickly sweet, even the eage...\n",
      "2      0  Gangs of New York is an unapologetic mess, who...\n",
      "3      0  We never really feel involved with the story, ...\n",
      "4      2              This is one of Polanski's best films.\n",
      "Number of rows: 1821\n"
     ]
    }
   ],
   "source": [
    "base_path = \"Finalv SST-2 dataset CSV format\"\n",
    "splits = [\"train.csv\", \"val.csv\", \"test.csv\"]\n",
    "\n",
    "frames = []\n",
    "for split in splits:\n",
    "    path = os.path.join(base_path, split)\n",
    "    print(f\"split: {splits}\")\n",
    "    df = show_data(path, label=\"label\", text=\"sentence\",name=\"SST-2\")\n",
    "    frames.append(df)\n",
    "\n",
    "sst_2 = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a12905-6f2f-4b22-8863-7abd7180d155",
   "metadata": {},
   "source": [
    "### SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93e6676e-96d2-496b-a485-7e80247812c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/azzouza2018/semevaldatadets\n"
     ]
    }
   ],
   "source": [
    "download_dataset('azzouza2018/semevaldatadets', 'semEval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eee26066-449d-4cda-8e45-957f3d8421eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: ['semeval-2013-dev.csv', 'semeval-2013-test.csv', 'semeval-2013-train.csv']\n",
      "Number of distinct labels: 3\n",
      "   label                                               text\n",
      "0      1  Watching Devil Inside for the 1st time tonight...\n",
      "1      1  @CMPunk Devil Inside , The exorcisism of Emily...\n",
      "2      1  Off to do my vlog. Watching Devil Inside and J...\n",
      "3      2  @raykipo take Silver at the Hib cup. Great day...\n",
      "4      1  @hollyhippo I'm going to blockbuster tomorrow ...\n",
      "Number of rows: 1650\n",
      "split: ['semeval-2013-dev.csv', 'semeval-2013-test.csv', 'semeval-2013-train.csv']\n",
      "Number of distinct labels: 3\n",
      "   label                                               text\n",
      "0      2  I just hope Ian Bennett is first out of the se...\n",
      "1      2  @JonHeymanCBS What is Selig's next brilliant i...\n",
      "2      1  When I looked at the full moon last night I im...\n",
      "3      2  Watched a movie yesterday #The70's on #OVTV an...\n",
      "4      2  I liked a @YouTube video http://t.co/dSPcjWDB ...\n",
      "Number of rows: 3545\n",
      "split: ['semeval-2013-dev.csv', 'semeval-2013-test.csv', 'semeval-2013-train.csv']\n",
      "Number of distinct labels: 3\n",
      "   label                                               text\n",
      "0      1  Today's news: Democrats offer Republicans ever...\n",
      "1      1  @stanscates I would use that in ads from now u...\n",
      "2      1  Microsoft issues critical patch for Windows 7,...\n",
      "3      1  will testdrive the new Nokia N9 phone with our...\n",
      "4      0  no way to underestimate the madness and cynici...\n",
      "Number of rows: 9616\n"
     ]
    }
   ],
   "source": [
    "base_path = \"semEval\"\n",
    "splits = [\"semeval-2013-dev.csv\", \"semeval-2013-test.csv\", \"semeval-2013-train.csv\"]\n",
    "\n",
    "frames = []\n",
    "for split in splits:\n",
    "    path = os.path.join(base_path, split)\n",
    "    print(f\"split: {splits}\")\n",
    "    df = show_data(path, \"label\", \"text\", delimiter='\\t', name = \"semEval\")\n",
    "    frames.append(df)\n",
    "\n",
    "sem_eval = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99f00a-b4e8-4da2-8501-092e0358cf43",
   "metadata": {},
   "source": [
    "## Concatenate Datasets and Inspect Combined DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c63909d-a789-4897-9a42-cb0fbabd768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74424\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 74424 entries, 0 to 14810\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    74424 non-null  object\n",
      " 1   label   74424 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "frames = [imdb, sst_2, sem_eval]\n",
    "df = pd.concat(frames)\n",
    "print(df.shape[0])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642ee4d-a8c4-49a7-bf73-0699cead72f1",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd896e75-5ab7-4320-954f-cacd61fda480",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5e17e37-9c72-45e1-b756-991a3e549a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aksinia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Aksinia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aksinia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aksinia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aksinia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Aksinia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Aksinia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8591a-0034-4cc6-99e0-c2cce233cbac",
   "metadata": {},
   "source": [
    "### Defining Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b6dbfd2-8c3b-480f-b7f1-0cb5aef49342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acef7e18-268e-4d3e-8b4a-b47fc29ad5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd734c0d-85a8-4cc5-bbe3-25fd9ed15872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hers', \"she's\", 'd', 'is', 'only', \"we'll\", 'again', \"should've\", 'shouldn', 'ours', 'into', 'the', 'shan', 'me', 'does', \"we're\", 'wouldn', 'it', \"mightn't\", 'themselves', 'now', \"i'll\", 've', 'after', 'who', 'once', \"you're\", \"doesn't\", 'by', 'needn', \"they're\", 'couldn', 'few', \"he's\", \"aren't\", 'yours', 'mightn', \"he'd\", 'how', 'through', 'them', 'between', 'himself', 'haven', 'herself', 'hadn', \"it'd\", 'ourselves', 'some', 'll', 'each', \"it'll\", 'there', \"hasn't\", 'her', 'same', 'if', 'than', \"he'll\", \"we'd\", \"they'll\", 'off', 'i', \"shan't\", 'doesn', 'ain', 'be', 't', 'y', 'over', \"they've\", 'being', 'our', \"shouldn't\", 'wasn', 'what', 'up', 'against', \"they'd\", \"you'd\", 'can', 'she', \"hadn't\", 'to', 'mustn', 'has', 'should', 'doing', 'which', 'am', 'from', \"i've\", 'don', 'your', 'didn', 'weren', 'under', 'in', 'its', 'both', 'm', \"i'd\", 'have', 'own', 'an', 'at', 'but', 'just', \"won't\", 'yourselves', 'other', 'until', 'o', \"haven't\", 'because', 'more', 'during', 'further', 's', 'yourself', \"i'm\", 'had', 'a', \"mustn't\", 'with', \"isn't\", \"don't\", \"needn't\", 'will', 'above', \"she'll\", 'here', 'so', 'his', 'any', 'hasn', 'most', 'of', 'that', 'before', 'these', \"she'd\", 'as', \"didn't\", \"wouldn't\", 'ma', 'such', \"you'll\", 'theirs', 'you', 'and', 'where', \"wasn't\", 'won', 'him', 'they', 'this', 'or', 'we', 'why', \"weren't\", 'all', 'been', 'itself', 'below', 'were', 're', 'while', \"you've\", 'myself', 'are', \"couldn't\", 'my', 'did', 'having', 'out', 'too', 'for', 'those', \"that'll\", 'when', 'down', 'isn', 'do', 'whom', 'he', 'then', 'their', 'very', 'aren', \"it's\", 'on', 'about', \"we've\", 'was'}\n"
     ]
    }
   ],
   "source": [
    "stop_words.discard('no')\n",
    "stop_words.discard('not')\n",
    "stop_words.discard('nor')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e425fbd4-9244-4730-b2bd-cba84e4881ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.MAX_VOCAB_SIZE = 10000\n",
    "        self.MAX_SEQ_LEN = 300\n",
    "        self.vocab = None\n",
    "\n",
    "    def get_tokens(self):\n",
    "\n",
    "        # Lowercase\n",
    "        self.df['text_processed'] = self.df.apply(lambda row: row['text'].lower(), axis=1)\n",
    "\n",
    "        # Remove anything that is NOT a word char or whitespace\n",
    "        self.df['text_processed'] = self.df.apply(lambda row: re.sub(r'[^\\w\\s]', '', row['text_processed']), axis=1)\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.df['tokens'] = self.df.apply(lambda row: nltk.word_tokenize(row['text_processed']), axis=1)\n",
    "\n",
    "    def get_terms(self):\n",
    "\n",
    "        #Remove stop words\n",
    "        self.df['terms'] = self.df.apply(lambda row: [t for t in row['tokens'] if t not in stop_words], axis=1)\n",
    "\n",
    "        #Linguistic modules\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        self.df['terms'] = self.df.apply(lambda row: [stemmer.stem(token) for token in row['terms']], axis=1)\n",
    "        # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        # self.df['terms'] = self.df.apply(lambda row: [lemmatizer.lemmatize(t, PreprocessDataset.get_wordnet_pos(t)) for t in row['terms']], axis=1)\n",
    "\n",
    "    def tokens_to_indices(self, tokens):\n",
    "        return [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "    def get_indexing(self):\n",
    "        \n",
    "        counter = Counter()\n",
    "        for tokens in self.df['terms']:\n",
    "            counter.update(tokens)\n",
    "            \n",
    "        self.vocab = {word: i+2 for i, (word, _) in enumerate(counter.most_common())}\n",
    "        self.vocab['<PAD>'] = 0\n",
    "        self.vocab['<UNK>'] = 1\n",
    "\n",
    "        self.df['input_ids'] = self.df['terms'].apply(lambda x: self.tokens_to_indices(x))\n",
    "\n",
    "    def pad(self, seq):\n",
    "        if len(seq) < self.MAX_SEQ_LEN:\n",
    "            return seq + [self.vocab['<PAD>']] * (self.MAX_SEQ_LEN - len(seq))\n",
    "        else:\n",
    "            return seq[:self.MAX_SEQ_LEN]\n",
    "\n",
    "    def add_padding(self):\n",
    "        \n",
    "        self.df['input_ids'] = self.df['input_ids'].apply(self.pad)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character for lemmatizer\"\"\"\n",
    "        from nltk import pos_tag\n",
    "        tag = pos_tag([word])[0][1][0].upper()\n",
    "        return {\n",
    "            'J': nltk.corpus.wordnet.ADJ,\n",
    "            'N': nltk.corpus.wordnet.NOUN,\n",
    "            'V': nltk.corpus.wordnet.VERB,\n",
    "            'R': nltk.corpus.wordnet.ADV\n",
    "        }.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "    def save_vocab(self, filepath):\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.vocab, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load_vocab(self, filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            self.vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95446108-dde2-4913-9277-3fff1044264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreprocessDataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b50c5-c3f3-43a6-8ea9-d8cc8f6347ab",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "Each token is a candidate for an index entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aeafe947-0546-4072-bd15-bbc09ad9d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [one, of, the, other, reviewers, has, mentione...\n",
      "1    [a, wonderful, little, production, br, br, the...\n",
      "2    [i, thought, this, was, a, wonderful, way, to,...\n",
      "3    [basically, theres, a, family, where, a, littl...\n",
      "4    [petter, matteis, love, in, the, time, of, mon...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor.get_tokens()\n",
    "print(preprocessor.df['tokens'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21569aae-bd9b-404a-80c2-ad1011432d72",
   "metadata": {},
   "source": [
    "### Terms\n",
    "\n",
    "Normalized word types (excluding stop words and after applying the linguistic module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b586509-4f74-4a1a-872b-dda0fd98931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.get_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a626da5-cdbb-4cda-87ad-3bc5d287bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [one, review, mention, watch, 1, oz, episod, y...\n",
      "1    [wonder, littl, product, br, br, film, techniq...\n",
      "2    [thought, wonder, way, spend, time, hot, summe...\n",
      "3    [basic, there, famili, littl, boy, jake, think...\n",
      "4    [petter, mattei, love, time, money, visual, st...\n",
      "Name: terms, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(preprocessor.df['terms'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656d409-580e-42ba-b6b3-3b4163868bf8",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Create vocabulary and assign unique integer IDs to each normalized term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c9fc9b6c-4098-4dca-bc75-dded9438889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [6, 249, 370, 13, 404, 2981, 199, 421, 1637, 1...\n",
      "1    [114, 55, 233, 2, 2, 4, 1666, 12728, 61868, 10...\n",
      "2    [108, 114, 37, 654, 8, 798, 1296, 1705, 442, 6...\n",
      "3    [403, 143, 145, 55, 238, 2869, 32, 143, 575, 3...\n",
      "4    [37858, 8398, 28, 8, 232, 510, 1124, 4, 13, 33...\n",
      "Name: input_ids, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor.get_indexing()\n",
    "print(preprocessor.df['input_ids'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3de79-91ee-42b4-aa72-e9f1ceb51450",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25bd10b8-cf4f-43d0-839e-dd53e7763280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [6, 249, 370, 13, 404, 2981, 199, 421, 1637, 1...\n",
      "1    [114, 55, 233, 2, 2, 4, 1666, 12728, 61868, 10...\n",
      "2    [108, 114, 37, 654, 8, 798, 1296, 1705, 442, 6...\n",
      "3    [403, 143, 145, 55, 238, 2869, 32, 143, 575, 3...\n",
      "4    [37858, 8398, 28, 8, 232, 510, 1124, 4, 13, 33...\n",
      "Name: input_ids, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor.add_padding()\n",
    "print(preprocessor.df['input_ids'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "71ce6211-83dd-47bb-8b0f-0c408dc35550",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.save_vocab('vocab.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90095610-1ae8-42ff-a69b-133a6c62181b",
   "metadata": {},
   "source": [
    "### Save preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b0fb3d1-246b-49b0-b54c-0b9554bdfed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists and will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('preprocessed_dataset.csv'):\n",
    "    print(\"File exists and will be overwritten.\")\n",
    "    \n",
    "preprocessor.df.to_csv('preprocessed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891c629-2e13-48b4-b7f4-e33bbebcdb56",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10497755-a50e-4191-94f9-6afd1b4b9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a361a9b8-20dd-4648-b192-530be65ed360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cf3724b-15f3-4a26-b226-1cea411d43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input_ids'] = df['input_ids'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59706bc-53c6-47c6-9862-6172c3bd649f",
   "metadata": {},
   "source": [
    "## Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5099e21d-e27e-4976-aaed-eb42cb28b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_path, vocab, embedding_dim=300):\n",
    "        embedding_matrix = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim)).astype(np.float32)\n",
    "        found = 0\n",
    "    \n",
    "        with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "    \n",
    "                if word in vocab:\n",
    "                    idx = vocab[word]\n",
    "                    embedding_matrix[idx] = vector\n",
    "                    found += 1\n",
    "    \n",
    "        print(f\"Found {found}/{len(vocab)} words in the embedding file.\")\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3dc5b0b-506e-4a36-bec1-7b62ddd07ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41311/161743 words in the embedding file.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(\"glove.6B.300d.txt\", preprocessor.vocab, embedding_dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf497b6-efa6-4e7c-a88b-b557f4b3594b",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a52c5c09-50d6-4b37-b815-4b4e1f01792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ce3c4d31-80cb-4dce-9b7f-0c4efc7a7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.X[idx], dtype=torch.long)\n",
    "        label_tensor = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return input_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f0cea-6299-4699-bc73-6a83e852e778",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4eb01f55-244a-40c3-975e-bfa695e9b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9434c112-6127-42bf-b25f-1b0c746298d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 59539\n",
      "Validation size: 7442\n",
      "Test size: 7443\n"
     ]
    }
   ],
   "source": [
    "X = df['input_ids'].tolist()\n",
    "y = df['label'].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3efbbc32-4af3-42ba-9201-20e79fd3da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "val_dataset = SentimentDataset(X_val, y_val)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers = 0, pin_memory  = False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], num_workers = 0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7da47f1-05c6-429b-8846-3478b101c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes    :  3\n",
      "No. of train samples :  59539\n",
      "Input shape example  :  torch.Size([300])\n",
      "Batch size           :  32\n",
      "Train batches        :  1861\n",
      "Val batches          :  233\n",
      "Test batches         :  233\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes    : \", len(set(y)))\n",
    "print(\"No. of train samples : \", len(train_dataset))\n",
    "print(\"Input shape example  : \", train_dataset[0][0].shape)\n",
    "print(\"Batch size           : \", config['batch_size'])\n",
    "print(\"Train batches        : \", len(train_loader))\n",
    "print(\"Val batches          : \", len(val_loader))\n",
    "print(\"Test batches         : \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc236179-471e-4d17-86fb-e7c70fd00aa9",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad52cb-98d5-452a-befb-98e66441c830",
   "metadata": {},
   "source": [
    "# TODO: experiemnt with embedding_matrix and  vocab_size learned from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5aa8bc8f-813c-40d2-9c85-0fb4cc7b049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional model because sometimes, the meaning of a word depends on what comes after it (not just before). \n",
    "#For example: in \"not good\", the word \"not\" completely flips the sentiment of \"good\"\n",
    "\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, output_dim=2, \n",
    "                 n_layers=2, bidirectional=True, dropout=0.5, pad_idx=0):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [B, T]\n",
    "        embedded = self.dropout(self.embedding(input_ids))       # [B, T, E]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)              # hidden: [n_layers*2, B, H]\n",
    "\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [B, H*2]\n",
    "        else:\n",
    "            hidden = hidden[-1]                                  # [B, H]\n",
    "        \n",
    "        out = self.fc(self.dropout(hidden))                      # [B, output_dim]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a35494e-bc13-493a-8223-34179a793401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentLSTM(\n",
    "    vocab_size=len(preprocessor.vocab), \n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim= df['label'].nunique(), \n",
    "    n_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pad_idx=preprocessor.vocab['<PAD>']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32705f70-01fc-46ae-820e-a432391869f9",
   "metadata": {},
   "source": [
    "## Setup everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "54f13bb9-9709-49e6-95bb-37cf29c310a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "# Smooth cosine-shaped LR decay, good when you train for a fixed number of epochs.\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"epochs\"])\n",
    "# You’re doing validation each epoch.\n",
    "# You want to reduce LR only when validation loss plateaus.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "12e3f1b9-80ac-46ca-8961-75403f9ae04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  80, 6541,  146, 1124,  708,   99,   20,   27,  251, 1419,  552,  618,\n",
      "          75,   65, 1799, 1295,  300,   27,  310,   85,   27,  209,  399,    3,\n",
      "          48,   55, 1621, 2125, 1614, 1888,  483, 1322,   87,  628,  855,   27,\n",
      "          65,  786,  133,   26,  214,  131, 2803,  103,   71,   37,   29,   69,\n",
      "         105,  101,  827,   27,  388,  123, 1339, 9409,  106, 1047,   16,  507,\n",
      "          16, 5789,   63,   80, 4698, 1197,   11,   93,  433,    3,  890,  111,\n",
      "         211,  554,  458,   14,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]), tensor(2))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7c6f0-8acb-4f01-a19f-5ad438e97538",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7597e12e-f2be-4688-a979-1ae094da73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, scaler=None, scheduler=None, config=None):\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Mixed precision context (optional)\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        num_correct += (preds == labels).sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=f\"{total_loss/(i+1):.4f}\",\n",
    "            acc=f\"{100 * num_correct / ((i+1)*inputs.size(0)):.2f}%\",\n",
    "            lr=f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "        batch_bar.update()\n",
    "\n",
    "        # Scheduler step if NOT ReduceLROnPlateau\n",
    "        if scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * num_correct / (len(dataloader.dataset))\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b76cf-c6e7-4df7-a9fe-b00b42e24311",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6fa370ee-771c-468b-ac13-4fe158422dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, config=None):\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Val')\n",
    "\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            num_correct += (preds == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                loss=f\"{total_loss/(i+1):.4f}\",\n",
    "                acc=f\"{100 * num_correct / ((i+1)*inputs.size(0)):.2f}%\"\n",
    "            )\n",
    "            batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * num_correct / (len(dataloader.dataset))\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eda68f48-f6e0-4093-b8e6-5b440000cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_loop(model, train_loader, val_loader, optimizer, criterion, device, config, scheduler=None, scaler=None, patience=5):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "        train_acc, train_loss = train(model, train_loader, optimizer, criterion, device, scaler, scheduler, config)\n",
    "        val_acc, val_loss = validate(model, val_loader, criterion, device, config)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # ReduceLROnPlateau scheduler step\n",
    "        if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Best model saved as best_model.pth.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "                break\n",
    "\n",
    "        # Clean up to avoid CUDA OOM\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6745b8f0-4925-412b-8005-f37a9d736d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0,\n",
      "        2, 1, 0, 0, 0, 2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    print(labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7416318-363a-45b5-8d84-7095e1001d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aksinia\\AppData\\Local\\Temp\\ipykernel_21476\\984290748.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(device)\n",
      "C:\\Users\\Aksinia\\anaconda3\\lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|                                                                                  | 0/1861 [00:00<?, ?it/s]C:\\Users\\Aksinia\\AppData\\Local\\Temp\\ipykernel_21476\\924483530.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
      "C:\\Users\\Aksinia\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7967, Train Acc: 56.02%\n",
      "Val   Loss: 0.5984, Val   Acc: 71.69%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6492, Train Acc: 69.98%\n",
      "Val   Loss: 0.5430, Val   Acc: 75.34%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5110, Train Acc: 77.33%\n",
      "Val   Loss: 0.4669, Val   Acc: 80.13%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4479, Train Acc: 80.62%\n",
      "Val   Loss: 0.4271, Val   Acc: 81.89%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4010, Train Acc: 83.24%\n",
      "Val   Loss: 0.4092, Val   Acc: 82.60%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3712, Train Acc: 84.75%\n",
      "Val   Loss: 0.4161, Val   Acc: 82.93%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3443, Train Acc: 85.90%\n",
      "Val   Loss: 0.4188, Val   Acc: 83.24%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3213, Train Acc: 87.04%\n",
      "Val   Loss: 0.4350, Val   Acc: 83.54%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2883, Train Acc: 88.43%\n",
      "Val   Loss: 0.4242, Val   Acc: 83.79%\n",
      "Learning Rate: 0.000500\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2750, Train Acc: 89.05%\n",
      "Val   Loss: 0.4239, Val   Acc: 83.89%\n",
      "Learning Rate: 0.000500\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Training complete.\n",
      "Best Validation Accuracy: 83.89%\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(device)\n",
    "\n",
    "train_full_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    scheduler=scheduler,\n",
    "    scaler=scaler,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648c00e-5167-40b1-9201-be875361eee0",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "72b91e86-3cbf-4056-893d-2510c2d19ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Test')\n",
    "\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            num_correct += (preds == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                loss=f\"{total_loss/(i+1):.4f}\",\n",
    "                acc=f\"{100 * num_correct / ((i+1)*inputs.size(0)):.2f}%\"\n",
    "            )\n",
    "            batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * num_correct / len(dataloader.dataset)\n",
    "\n",
    "    print(f\"\\nTest Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy, avg_loss, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0ae16312-a116-4a36-aea1-30c617d69049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.4056, Test Accuracy: 84.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Then call the test function\n",
    "test_accuracy, test_loss, predictions, ground_truth = test(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abb456-d2da-4ec2-bee9-01d80ac59d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
