{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cde010a-2b60-4a1b-8e7d-e6d124548240",
   "metadata": {},
   "source": [
    "# Project 4: Sentiment Analysis of Text\n",
    "\n",
    "This project involves building and evaluating deep learning models (RNNs or Transformers) for sentiment classification of text, such as movie reviews or product feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661f799-04cc-477d-a497-4267ff78d464",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f43da2d-8a7f-4a5e-bb61-ef186639d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848d9e3-fc5a-47ff-b29c-037a079cb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python & Data Handling\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch Core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# For Reproducibility (optional but recommended)\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# For summary of model\n",
    "\n",
    "\n",
    "# for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3390a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774877db-1d9a-4a1e-99fb-c0420aca9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79038126-4c30-4196-ba85-82ee0f67ce48",
   "metadata": {},
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4e2cb1-a156-43d9-9b6c-8a13e216b5f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkaggle_api_extended\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleApi\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kaggle'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196c1cf7-c36d-49be-b168-204b1241eb1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KaggleApi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Setup\u001b[39;00m\n\u001b[32m      2\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mKAGGLE_CONFIG_DIR\u001b[39m\u001b[33m'\u001b[39m] = os.getcwd()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m api = \u001b[43mKaggleApi\u001b[49m()\n\u001b[32m      4\u001b[39m api.authenticate()\n",
      "\u001b[31mNameError\u001b[39m: name 'KaggleApi' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d98439-c7ac-45fd-8fc1-b74c047042c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(path: str, label: str, text: str, delimiter= \",\"):\n",
    "    df = pd.read_csv(path,delimiter=delimiter)\n",
    "\n",
    "    df = df.rename(columns={text: \"text\", label: \"label\"})\n",
    "\n",
    "    if not df['label'].isin([0, 1, -1]).all():\n",
    "        df['label'] = df['label'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "    num_labels = df['label'].nunique()\n",
    "    print(f\"Number of distinct labels: {num_labels}\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd423-fee5-45d3-b413-abe4b2e47180",
   "metadata": {},
   "source": [
    "## IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d20b5c-1361-42d1-99f1-8e2a6f01e206",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dataset = \u001b[33m'\u001b[39m\u001b[33mlakshmi25npathi/imdb-dataset-of-50k-movie-reviews\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mapi\u001b[49m.dataset_download_files(dataset, path= \u001b[33m'\u001b[39m\u001b[33mIMDB\u001b[39m\u001b[33m'\u001b[39m, unzip=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = 'lakshmi25npathi/imdb-dataset-of-50k-movie-reviews'\n",
    "api.dataset_download_files(dataset, path= 'IMDB', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3144023-ebb1-4fca-9693-d16ca68d2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct labels: 2\n",
      "                                                text  label\n",
      "0  One of the other reviewers has mentioned that ...      1\n",
      "1  A wonderful little production. <br /><br />The...      1\n",
      "2  I thought this was a wonderful way to spend ti...      1\n",
      "3  Basically there's a family where a little boy ...      0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...      1\n",
      "Number of rows: 50000\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\"IMDB\", \"IMDB Dataset.csv\")\n",
    "imdb = show_data(path, \"sentiment\", \"review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9adc7-b04c-4f0e-ae45-58ea584ea0fb",
   "metadata": {},
   "source": [
    "## SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4d87da9-537a-469a-b325-ebbeb897b1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/jkhanbk1/sst2-dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = 'jkhanbk1/sst2-dataset'\n",
    "api.dataset_download_files(dataset, path='.', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349e92b9-3b1f-47d2-914d-82deb2d7678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct labels: 2\n",
      "   label                                               text\n",
      "0      0        No movement, no yuks, not much of anything.\n",
      "1      0  A gob of drivel so sickly sweet, even the eage...\n",
      "2      0  Gangs of New York is an unapologetic mess, who...\n",
      "3      0  We never really feel involved with the story, ...\n",
      "4      1              This is one of Polanski's best films.\n",
      "Number of rows: 1821\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\"Finalv SST-2 dataset CSV format\", \"test.csv\")\n",
    "sst_2 = show_data(path, \"label\", \"sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a12905-6f2f-4b22-8863-7abd7180d155",
   "metadata": {},
   "source": [
    "## SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93e6676e-96d2-496b-a485-7e80247812c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/azzouza2018/semevaldatadets\n"
     ]
    }
   ],
   "source": [
    "dataset = 'azzouza2018/semevaldatadets'\n",
    "api.dataset_download_files(dataset, path='semEval', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee26066-449d-4cda-8e45-957f3d8421eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct labels: 3\n",
      "   label                                               text\n",
      "0      0  Watching Devil Inside for the 1st time tonight...\n",
      "1      0  @CMPunk Devil Inside , The exorcisism of Emily...\n",
      "2      0  Off to do my vlog. Watching Devil Inside and J...\n",
      "3      1  @raykipo take Silver at the Hib cup. Great day...\n",
      "4      0  @hollyhippo I'm going to blockbuster tomorrow ...\n",
      "Number of rows: 1650\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\"semEval\", \"semeval-2013-dev.csv\")\n",
    "sem_eval= show_data(path, \"label\", \"text\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c63909d-a789-4897-9a42-cb0fbabd768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53471\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 53471 entries, 0 to 1649\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    53471 non-null  object\n",
      " 1   label   53471 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "frames = [imdb, sst_2, sem_eval]\n",
    "df = pd.concat(frames)\n",
    "print(df.shape[0])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642ee4d-a8c4-49a7-bf73-0699cead72f1",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd896e75-5ab7-4320-954f-cacd61fda480",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e17e37-9c72-45e1-b756-991a3e549a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\garik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\garik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\garik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\garik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\garik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\garik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\garik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8591a-0034-4cc6-99e0-c2cce233cbac",
   "metadata": {},
   "source": [
    "### Defining Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acef7e18-268e-4d3e-8b4a-b47fc29ad5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd734c0d-85a8-4cc5-bbe3-25fd9ed15872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"haven't\", 'had', 'below', 'does', 'as', 'their', 'what', 'm', \"should've\", 'or', 'will', 'he', 'herself', 'most', 'needn', 'from', 'aren', 'under', 'other', 'how', 'wasn', 'over', 'an', 'themselves', 'didn', 'until', 'll', 'this', 'each', 'having', 'ma', \"mightn't\", 'those', \"shan't\", \"she'd\", 'the', \"don't\", 're', 'by', 'because', 'has', 'y', 'now', \"it'd\", 'weren', 'too', 'me', 'was', 'between', 'out', 'whom', \"shouldn't\", 'more', 'only', 'your', 'off', 'then', 'so', 'few', 'd', 'but', 'where', 'i', 'myself', \"she'll\", \"he's\", \"that'll\", \"weren't\", \"she's\", 'who', \"i'll\", 'mightn', 'very', \"we'll\", 'just', 'it', \"you'll\", 'ours', 'did', 'her', \"you've\", 'while', 'mustn', \"doesn't\", 'o', \"won't\", 'wouldn', 'at', 'above', 'on', 'a', 'is', 'have', 'his', 'our', 'isn', \"wasn't\", 'further', \"they've\", 'why', \"you're\", 'him', 'through', 'himself', \"couldn't\", 'hasn', 'all', 'can', \"i've\", 'shan', 'itself', 'once', 'same', \"they'd\", \"we'd\", \"i'm\", 'of', 'when', 'haven', 'ourselves', 'they', 'its', \"we've\", 'yours', 'we', 'own', \"they're\", 'yourself', \"aren't\", 'before', 'were', 'been', \"we're\", 'do', 'for', 'with', 'about', 'some', \"you'd\", 'than', 'again', \"isn't\", 'theirs', 'in', \"they'll\", 'any', 'after', 'both', 'hadn', \"didn't\", 'yourselves', \"it's\", \"needn't\", 'to', 'if', 'doesn', 'couldn', 'don', 'won', 'ain', 'such', 'against', \"mustn't\", 's', 'there', 'shouldn', 've', 'here', 'them', 'that', 'are', 'doing', \"wouldn't\", 't', 'up', 'which', 'being', 'these', 'and', \"he'll\", 'should', 'into', 'be', 'during', \"he'd\", 'she', 'down', \"i'd\", 'my', 'am', \"it'll\", 'hers', \"hadn't\", 'you', \"hasn't\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words.discard('no')\n",
    "stop_words.discard('not')\n",
    "stop_words.discard('nor')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425fbd4-9244-4730-b2bd-cba84e4881ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.MAX_VOCAB_SIZE = 10000\n",
    "        self.MAX_SEQ_LEN = 300\n",
    "        self.vocab = None\n",
    "\n",
    "    def get_tokens(self):\n",
    "\n",
    "        # Lowercase\n",
    "        self.df['text_processed'] = self.df.apply(lambda row: row['text'].lower(), axis=1)\n",
    "\n",
    "        # Remove anything that is NOT a word char or whitespace\n",
    "        self.df['text_processed'] = self.df.apply(lambda row: re.sub(r'[^\\w\\s]', '', row['text_processed']), axis=1)\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.df['tokens'] = self.df.apply(lambda row: nltk.word_tokenize(row['text_processed']), axis=1)\n",
    "\n",
    "    # МОЖНО ВЫБРАТЬ ЛИНГВИСТИЧЕСКУЮ МОДЕЛЬ\n",
    "    def get_terms(self):\n",
    "\n",
    "        #Remove stop words\n",
    "        self.df['terms'] = self.df.apply(lambda row: [t for t in row['tokens'] if t not in stop_words], axis=1)\n",
    "\n",
    "        #Linguistic modules\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        self.df['terms'] = self.df.apply(lambda row: [stemmer.stem(token) for token in row['terms']], axis=1)\n",
    "        # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        # self.df['terms'] = self.df.apply(lambda row: [lemmatizer.lemmatize(t, PreprocessDataset.get_wordnet_pos(t)) for t in row['terms']], axis=1)\n",
    "\n",
    "    def tokens_to_indices(self, tokens):\n",
    "        return [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "\n",
    "    def get_indexing(self):\n",
    "        \n",
    "        counter = Counter()\n",
    "        for tokens in self.df['terms']:\n",
    "            counter.update(tokens)\n",
    "            \n",
    "        self.vocab = {word: i+2 for i, (word, _) in enumerate(counter.most_common())}\n",
    "        self.vocab['<PAD>'] = 0\n",
    "        self.vocab['<UNK>'] = 1\n",
    "\n",
    "        self.df['input_ids'] = self.df['terms'].apply(lambda x: self.tokens_to_indices(x))\n",
    "\n",
    "    def pad(self, seq):\n",
    "        if len(seq) < self.MAX_SEQ_LEN:\n",
    "            return seq + [self.vocab['<PAD>']] * (self.MAX_SEQ_LEN - len(seq))\n",
    "        else:\n",
    "            return seq[:self.MAX_SEQ_LEN]\n",
    "\n",
    "    def add_padding(self):\n",
    "        \n",
    "        self.df['input_ids'] = self.df['input_ids'].apply(self.pad)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character for lemmatizer\"\"\"\n",
    "        from nltk import pos_tag\n",
    "        tag = pos_tag([word])[0][1][0].upper()\n",
    "        return {\n",
    "            'J': nltk.corpus.wordnet.ADJ,\n",
    "            'N': nltk.corpus.wordnet.NOUN,\n",
    "            'V': nltk.corpus.wordnet.VERB,\n",
    "            'R': nltk.corpus.wordnet.ADV\n",
    "        }.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "    def save_vocab(self, filepath):\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.vocab, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load_vocab(self, filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            self.vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95446108-dde2-4913-9277-3fff1044264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = PreprocessDataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b50c5-c3f3-43a6-8ea9-d8cc8f6347ab",
   "metadata": {},
   "source": [
    "Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeafe947-0546-4072-bd15-bbc09ad9d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [one, of, the, other, reviewers, has, mentione...\n",
      "1    [a, wonderful, little, production, br, br, the...\n",
      "2    [i, thought, this, was, a, wonderful, way, to,...\n",
      "3    [basically, theres, a, family, where, a, littl...\n",
      "4    [petter, matteis, love, in, the, time, of, mon...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_dataset.get_tokens()\n",
    "print(df_dataset.df['tokens'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21569aae-bd9b-404a-80c2-ad1011432d72",
   "metadata": {},
   "source": [
    "Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b586509-4f74-4a1a-872b-dda0fd98931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.get_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a626da5-cdbb-4cda-87ad-3bc5d287bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [one, review, mention, watch, 1, oz, episod, y...\n",
      "1    [wonder, littl, product, br, br, film, techniq...\n",
      "2    [thought, wonder, way, spend, time, hot, summe...\n",
      "3    [basic, there, famili, littl, boy, jake, think...\n",
      "4    [petter, mattei, love, time, money, visual, st...\n",
      "Name: terms, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_dataset.df['terms'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c06c5c-8448-4815-adc4-6c521ff9432a",
   "metadata": {},
   "source": [
    "Create vocabulary and Convert Text to Sequences of Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9fc9b6c-4098-4dca-bc75-dded9438889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [6, 241, 364, 14, 429, 2885, 199, 416, 1621, 1...\n",
      "1    [112, 56, 230, 2, 2, 4, 1632, 13396, 58922, 11...\n",
      "2    [104, 112, 37, 658, 8, 824, 1335, 2092, 440, 6...\n",
      "3    [393, 144, 146, 56, 238, 2829, 32, 144, 562, 3...\n",
      "4    [58924, 8616, 31, 8, 228, 517, 1090, 4, 14, 33...\n",
      "Name: input_ids, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_dataset.get_indexing()\n",
    "print(df_dataset.df['input_ids'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3de79-91ee-42b4-aa72-e9f1ceb51450",
   "metadata": {},
   "source": [
    "Add pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25bd10b8-cf4f-43d0-839e-dd53e7763280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [6, 241, 364, 14, 429, 2885, 199, 416, 1621, 1...\n",
      "1    [112, 56, 230, 2, 2, 4, 1632, 13396, 58922, 11...\n",
      "2    [104, 112, 37, 658, 8, 824, 1335, 2092, 440, 6...\n",
      "3    [393, 144, 146, 56, 238, 2829, 32, 144, 562, 3...\n",
      "4    [58924, 8616, 31, 8, 228, 517, 1090, 4, 14, 33...\n",
      "Name: input_ids, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_dataset.add_padding()\n",
    "print(df_dataset.df['input_ids'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71ce6211-83dd-47bb-8b0f-0c408dc35550",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdataset\u001b[49m.save_vocab(\u001b[33m'\u001b[39m\u001b[33mvocab.json\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.save_vocab('vocab.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90095610-1ae8-42ff-a69b-133a6c62181b",
   "metadata": {},
   "source": [
    "Save preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b0fb3d1-246b-49b0-b54c-0b9554bdfed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists and will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('preprocessed_dataset.csv'):\n",
    "    print(\"File exists and will be overwritten.\")\n",
    "    \n",
    "df_dataset.df.to_csv('preprocessed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891c629-2e13-48b4-b7f4-e33bbebcdb56",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10497755-a50e-4191-94f9-6afd1b4b9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a361a9b8-20dd-4648-b192-530be65ed360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cf3724b-15f3-4a26-b226-1cea411d43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input_ids'] = df['input_ids'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faab1609-b841-4240-95b2-cb8fa9419032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_label(label):\n",
    "    if label == -1:\n",
    "        return 0\n",
    "    elif label == 0:\n",
    "        return 1\n",
    "    elif label == 1:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fb22040-b2a1-4c2e-b0c1-934efbf15a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(remap_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf497b6-efa6-4e7c-a88b-b557f4b3594b",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a52c5c09-50d6-4b37-b815-4b4e1f01792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce3c4d31-80cb-4dce-9b7f-0c4efc7a7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X  # Keep as a list of lists\n",
    "        self.y = y  # Also keep as list or array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.X[idx], dtype=torch.long)\n",
    "        label_tensor = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return input_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f0cea-6299-4699-bc73-6a83e852e778",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4eb01f55-244a-40c3-975e-bfa695e9b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 32, # Number of samples per training step, how many samples my model sees at once befor update weights\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9434c112-6127-42bf-b25f-1b0c746298d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 42776\n",
      "Validation size: 5347\n",
      "Test size: 5348\n"
     ]
    }
   ],
   "source": [
    "X = df['input_ids'].tolist()\n",
    "y = df['label'].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3efbbc32-4af3-42ba-9201-20e79fd3da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "val_dataset = SentimentDataset(X_val, y_val)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers = 0, pin_memory  = False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], num_workers = 0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7da47f1-05c6-429b-8846-3478b101c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes    :  3\n",
      "No. of train samples :  42776\n",
      "Input shape example  :  torch.Size([300])\n",
      "Batch size           :  32\n",
      "Train batches        :  1337\n",
      "Val batches          :  168\n",
      "Test batches         :  168\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes    : \", len(set(y)))\n",
    "print(\"No. of train samples : \", len(train_dataset))\n",
    "print(\"Input shape example  : \", train_dataset[0][0].shape)\n",
    "print(\"Batch size           : \", config['batch_size'])\n",
    "print(\"Train batches        : \", len(train_loader))\n",
    "print(\"Val batches          : \", len(val_loader))\n",
    "print(\"Test batches         : \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc236179-471e-4d17-86fb-e7c70fd00aa9",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa8bc8f-813c-40d2-9c85-0fb4cc7b049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional model because sometimes, the meaning of a word depends on what comes after it (not just before). \n",
    "#For example: in \"not good\", the word \"not\" completely flips the sentiment of \"good\"\n",
    "\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, output_dim=2, \n",
    "                 n_layers=2, bidirectional=True, dropout=0.5, pad_idx=0):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [B, T]\n",
    "        embedded = self.dropout(self.embedding(input_ids))       # [B, T, E]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)              # hidden: [n_layers*2, B, H]\n",
    "\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [B, H*2]\n",
    "        else:\n",
    "            hidden = hidden[-1]                                  # [B, H]\n",
    "        \n",
    "        out = self.fc(self.dropout(hidden))                      # [B, output_dim]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a35494e-bc13-493a-8223-34179a793401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentLSTM(\n",
    "    vocab_size=len(df_dataset.vocab), \n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    output_dim= df['label'].nunique(), \n",
    "    n_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    "    pad_idx=df_dataset.vocab['<PAD>']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b46890-0207-4651-a04c-794a17760e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(df_dataset.MAX_SEQ_LEN,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d749d2e2-111f-46e0-a708-5ff257cd1131",
   "metadata": {},
   "source": [
    "# Setup everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54f13bb9-9709-49e6-95bb-37cf29c310a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garik\\Desktop\\asdsad\\Sentiment_Analysis_of_Text\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "# Smooth cosine-shaped LR decay, good when you train for a fixed number of epochs.\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"epochs\"])\n",
    "# You’re doing validation each epoch.\n",
    "# You want to reduce LR only when validation loss plateaus.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12e3f1b9-80ac-46ca-8961-75403f9ae04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([    26,     14,      3,    153,    263,      6,     48,   1196,    130,\n",
      "          5403,    177,      6,    771,    188,    172,     25,     22,   1145,\n",
      "          1661,    173,      5,      3,   1021,    287,    163,   4394,    803,\n",
      "            24,     61,   4208,    119,    965,    245,    183,   1403,   4729,\n",
      "           361,   7144,     28,   1958,   1273,    113,   4997,   4967,    173,\n",
      "           444,   9157,     91,    863,    105,     67,    287, 126872,    385,\n",
      "           414,     88,    371,   1619,   5042,      3,  40010,     15,     30,\n",
      "            30,      9,     37,     30,    198,     37,   1336,   5297,      4,\n",
      "           327,      3,     70,    273,    416,    493,    246,   1375,   1281,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7c6f0-8acb-4f01-a19f-5ad438e97538",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7597e12e-f2be-4688-a979-1ae094da73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, scaler=None, scheduler=None, config=None):\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Mixed precision context (optional)\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        num_correct += (preds == labels).sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=f\"{total_loss/(i+1):.4f}\",\n",
    "            acc=f\"{100 * num_correct / ((i+1)*inputs.size(0)):.2f}%\",\n",
    "            lr=f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "        batch_bar.update()\n",
    "\n",
    "        # Scheduler step if NOT ReduceLROnPlateau\n",
    "        if scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * num_correct / (len(dataloader.dataset))\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b76cf-c6e7-4df7-a9fe-b00b42e24311",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fa370ee-771c-468b-ac13-4fe158422dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, config=None):\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Val')\n",
    "\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            num_correct += (preds == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                loss=f\"{total_loss/(i+1):.4f}\",\n",
    "                acc=f\"{100 * num_correct / ((i+1)*inputs.size(0)):.2f}%\"\n",
    "            )\n",
    "            batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * num_correct / (len(dataloader.dataset))\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda68f48-f6e0-4093-b8e6-5b440000cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_loop(model, train_loader, val_loader, optimizer, criterion, device, config, scheduler=None, scaler=None, patience=5):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "        train_acc, train_loss = train(model, train_loader, optimizer, criterion, device, scaler, scheduler, config)\n",
    "        val_acc, val_loss = validate(model, val_loader, criterion, device, config)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # ReduceLROnPlateau scheduler step\n",
    "        if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Best model saved as best_model.pth.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "                break\n",
    "\n",
    "        # Clean up to avoid CUDA OOM\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6745b8f0-4925-412b-8005-f37a9d736d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1,\n",
      "        1, 2, 1, 1, 2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for inputs, labels in train_loader:\n",
    "    print(labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7416318-363a-45b5-8d84-7095e1001d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garik\\Desktop\\asdsad\\Sentiment_Analysis_of_Text\\.venv\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/1337 [00:00<?, ?it/s]c:\\Users\\garik\\Desktop\\asdsad\\Sentiment_Analysis_of_Text\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6643, Train Acc: 63.37%\n",
      "Val   Loss: 0.5230, Val   Acc: 77.00%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4956, Train Acc: 77.57%\n",
      "Val   Loss: 0.3939, Val   Acc: 83.00%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3925, Train Acc: 83.13%\n",
      "Val   Loss: 0.3372, Val   Acc: 85.56%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3384, Train Acc: 85.94%\n",
      "Val   Loss: 0.3127, Val   Acc: 86.85%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3003, Train Acc: 87.63%\n",
      "Val   Loss: 0.3314, Val   Acc: 87.13%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2719, Train Acc: 89.08%\n",
      "Val   Loss: 0.2998, Val   Acc: 87.97%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2459, Train Acc: 90.23%\n",
      "Val   Loss: 0.2924, Val   Acc: 87.82%\n",
      "Learning Rate: 0.001000\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2228, Train Acc: 91.28%\n",
      "Val   Loss: 0.3157, Val   Acc: 88.03%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2062, Train Acc: 91.78%\n",
      "Val   Loss: 0.3201, Val   Acc: 88.16%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1876, Train Acc: 92.76%\n",
      "Val   Loss: 0.3179, Val   Acc: 88.31%\n",
      "Learning Rate: 0.001000\n",
      "Best model saved as best_model.pth.\n",
      "\n",
      "Training complete.\n",
      "Best Validation Accuracy: 88.31%\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(device)\n",
    "\n",
    "train_full_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    scheduler=scheduler,\n",
    "    scaler=scaler,\n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f758fd",
   "metadata": {},
   "source": [
    "# Matplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab519af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_metrics(cfg, train_losses, val_losses, val_accuracies, val_f1s, test_loss, test_accuracy, test_f1):\n",
    "    \"\"\"\n",
    "    Plots and saves the training/validation loss, accuracy, and F1 score over epochs.\n",
    "    Optionally, plots test loss, accuracy, and F1 as horizontal lines.\n",
    "    \"\"\"\n",
    "    epochs = range(1, cfg['epochs'] + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.plot(epochs, val_f1s, label=\"Validation F1\")\n",
    "    plt.axhline(y=test_accuracy, color='g', linestyle='--', label='Test Accuracy')\n",
    "    plt.axhline(y=test_f1, color='m', linestyle='--', label='Test F1')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Validation Accuracy and F1 over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"performance_graph.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648c00e-5167-40b1-9201-be875361eee0",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b91e86-3cbf-4056-893d-2510c2d19ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Test')\n",
    "\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            num_correct += (preds == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                loss=f\"{total_loss/(i+1):.4f}\",\n",
    "                acc=f\"{100 * num_correct / ((i+1)*inputs.size(0)):.2f}%\"\n",
    "            )\n",
    "            batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * num_correct / len(dataloader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"\\nTest Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%, Test F1: {f1:.4f}\")\n",
    "    return accuracy, avg_loss, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae16312-a116-4a36-aea1-30c617d69049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.3264, Test Accuracy: 88.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Then call the test function\n",
    "test_accuracy, test_loss, predictions, ground_truth = test(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abb456-d2da-4ec2-bee9-01d80ac59d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\n",
    "    cfg=config,\n",
    "    train_losses=\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389e996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
